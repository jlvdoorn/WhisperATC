{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Whisper on ANSP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = 'ansp_data/audio'\n",
    "mdl = 'openai/whisper-large-v2'\n",
    "wsp = '-'.join(mdl.split('-')[1:])\n",
    "\n",
    "print('Dataset: ', dts)\n",
    "print('Model  : ', mdl)\n",
    "print('Whisper: ', wsp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_files = glob.glob(dts+'/*.wav')\n",
    "if len(wav_files) == 0:\n",
    "    raise Exception('No wav files found. Please check the path.')\n",
    "else:\n",
    "    print('Found {} audio files'.format(len(wav_files)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['hyp-prmpt', 'hyp-clean', 'ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model('-'.join(mdl.split('-')[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting inference...')\n",
    "time_start = datetime.time\n",
    "nato = \"alpha,bravo,charlie,delta,echo,foxtrot,golf,hotel,india,juliett,kilo,lima,mike,november,oscar,papa,quebec,romeo,sierra,tango,uniform,victor,whiskey,xray,yankee,zulu\"\n",
    "terminology = \"climb, climbing, descend, descending, passing, feet, knots, degrees, direct, maintain, identified, ILS, VFR, IFR, contact, frequency, turn, right, left, heading, altitude, flight, level, cleared, squawk, approach, runway, established, report, affirm, negative, wilco, roger, radio, radar, right, left, center\"\n",
    "sids = \"BERGI WISPA ANDIK BETUS NOPSU SPY TORGA ARNEM ELPAT NYKER EDUPO IVLUT RENDI LOPIK OGINA ROVEN KUDAD LARAS WOODY IDRID VOLLA\"\n",
    "\n",
    "for file in wav_files:\n",
    "    prompt = 'Air Traffic Control Communications ' + sids.replace(',',' ') + ' ' + nato.replace(',',' ') + ' ' + terminology.replace(',',' ')\n",
    "    \n",
    "    res_prmpt = model.transcribe(file, initial_prompt=prompt, language='en', fp16=False)\n",
    "    res_clean = model.transcribe(file, language='en', fp16=False)\n",
    "    df.loc[len(df.index)] = [res_prmpt['text'], res_clean['text'], ' ']\n",
    "    \n",
    "    i = wav_files.index(file)+1\n",
    "    print('Inference: {:.3f} %'.format(i/len(wav_files)*100), end='\\r')\n",
    "\n",
    "time_end = datetime.time\n",
    "print('Finished {} files in {:.2f} seconds'.format(len(wav_files), (time_end-time_start)/60))\n",
    "df.to_excel('ANSP-'+mdl.split('/')[-1]+'-'+datetime.today().strftime('%Y-%m-%d--%H:%M:%S')+'.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "normalizer = EnglishTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "nato_alphabet_mapping       = {'A': 'alpha', 'B': 'bravo', 'C': 'charlie', 'D': 'delta', 'E': 'echo', \n",
    "                            'F': 'foxtrot', 'G': 'golf', 'H': 'hotel', 'I': 'india', 'J': 'juliett',\n",
    "                            'K': 'kilo', 'L': 'lima', 'M': 'mike', 'N': 'november', 'O': 'oscar',\n",
    "                            'P': 'papa', 'Q': 'quebec', 'R': 'romeo', 'S': 'sierra', 'T': 'tango',\n",
    "                            'U': 'uniform', 'V': 'victor', 'W': 'whiskey', 'X': 'xray', 'Y': 'yankee', 'Z': 'zulu',\n",
    "                         \n",
    "                            '1': 'one', '2': 'two', '3': 'three', '4': 'four', '5': 'five',\n",
    "                            '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine', '10': 'ten', \n",
    "                            '0': 'zero', '00': 'hundred', '000': 'thousand',\n",
    "                         \n",
    "                            '.': 'decimal', ',': 'comma', '-': 'dash',}\n",
    "nato_similarities           = {'alfa': 'alpha', 'oskar': 'oscar', 'ekko': 'echo', 'gulf': 'golf'}\n",
    "terminology_mapping         = {'FL': 'flight level'}\n",
    "text_similarities           = {'descent': 'descend'}\n",
    "\n",
    "# Not needed for WER calculations\n",
    "# airlines_icao_mapping       = {'lufthansa': 'lufthansa', 'speedbird': 'british airways'}\n",
    "# airlines_synonym_mapping    = {'hansa': 'lufthansa'}\n",
    "\n",
    "# Sometimes Whisper is intelligent enough to perceive 'eurowings seven alpha bravo' as 'EW7AB'\n",
    "airlines_iata_codes         = {'BA': 'british airways', 'KL': 'klm', 'LH': 'lufthansa', 'EW': 'eurowings'}\n",
    "airlines_icao_codes         = {'BAW': 'british airways', 'DLH': 'lufthansa', 'KLM': 'klm', 'EWG': 'eurowings'}\n",
    "\n",
    "def aerospaceTransform(text):\n",
    "    wrds = text.split()\n",
    "    for word in wrds:\n",
    "        if word in nato_alphabet_mapping:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = nato_alphabet_mapping[word]\n",
    "        if word.lower() in nato_similarities:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = nato_similarities[word.lower()]\n",
    "        if word in terminology_mapping:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = terminology_mapping[word]\n",
    "        if word.lower() in text_similarities:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = text_similarities[word.lower()]\n",
    "        if word.upper() in airlines_iata_codes:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = airlines_iata_codes[word.upper()]            \n",
    "        if word.upper() in airlines_icao_codes:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = airlines_icao_codes[word.upper()]\n",
    "    return ' '.join(wrds)\n",
    "\n",
    "normalizer = EnglishTextNormalizer()\n",
    "\n",
    "def removePunctuation(text):\n",
    "    text = ''.join(\n",
    "        ' ' if c in '!@#$%^&*~-+=_\\|;:,.?' else c\n",
    "        for c in text\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def separateNumbersAndText(text):\n",
    "    text = re.split('(\\d+)', text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def separateCallSignLetters(text):\n",
    "    wrds = text.split()\n",
    "    prohibited_words = ['ILS', 'IFR', 'FL']\n",
    "    for word in wrds:\n",
    "        if word.isupper() and word not in prohibited_words:\n",
    "            ltrs = [str(l) for l in word]\n",
    "            ltrs = ' '.join(str(l) for l in ltrs)\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = ltrs\n",
    "    \n",
    "    return ' '.join(wrds)\n",
    "\n",
    "def splitNumbersIntoDigits(text):\n",
    "    wrds = text.split()\n",
    "    for word in wrds:\n",
    "        if word.isnumeric():\n",
    "            dgts = [int(d) for d in word]\n",
    "            dgts = ' '.join(str(d) for d in dgts)\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = dgts\n",
    "        \n",
    "    return ' '.join(wrds)\n",
    "\n",
    "def removeSpokenSeparators(text):\n",
    "    wrds = text.split()\n",
    "    for word in wrds:\n",
    "        if word.lower() in ['decimal', 'comma', 'point']:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = ''\n",
    "        \n",
    "    return ' '.join(wrds)\n",
    "\n",
    "def splitGreetings(text):\n",
    "    wrds = text.split()\n",
    "    for word in wrds:\n",
    "        if word.lower() in ['goodbye']:\n",
    "            x = wrds.index(word)\n",
    "            wrds[x] = 'good bye'\n",
    "            \n",
    "    return ' '.join(wrds)\n",
    "\n",
    "def removeCharSet(text, c1, c2): # for removing all text within (and including) a character set (ex.: [TRANSCRIPT] )\n",
    "    while c1 in text and c2 in text:\n",
    "        x = text.find(c1)\n",
    "        y = text.rfind(c2) # Should be the last entry of the closing element ) ] > \n",
    "        text = text[0:x] + text[y+1:]\n",
    "    return text\n",
    "\n",
    "def removeChar(text, c1): # for removing a single character (ex.: @ )\n",
    "    while c1 in text:\n",
    "        x = text.find(c1)\n",
    "        text = text[0:x] + text[x+1:]\n",
    "    return text\n",
    "\n",
    "def removeNonAlphaNum(text): # for removing all non alphanumeric characters (ex.: ! @ # $ % ^ & * ) (AlphanNum.: A-Z, a-z, 0-9)\n",
    "    for c in text:\n",
    "        if c.isalnum() == False and c != ' ' :\n",
    "            x = text.find(c)\n",
    "            text = text[0:x] + text[x+1:]\n",
    "    return text\n",
    "\n",
    "def filterAndNormalize(text):   \n",
    "    text = removeCharSet(text, '[', ']')\n",
    "    text = removeCharSet(text, '<', '>')\n",
    "    #text = removeCharSet(text, '(', ')')\n",
    "    \n",
    "    text = removeNonAlphaNum(text)\n",
    "    text = separateNumbersAndText(text)\n",
    "    text = aerospaceTransform(text)\n",
    "    text = removeSpokenSeparators(text)\n",
    "    # text = separateCallSignLetters(text)\n",
    "\n",
    "    text = normalizer(text)\n",
    "    text = normalizer(text)\n",
    "    # Running twice because the normalizer will replace 'zero five' by '05' but also replaces '05' by '5' (removing leading zeros).\n",
    "    \n",
    "    text = splitNumbersIntoDigits(text)\n",
    "\n",
    "    text = splitGreetings(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def normalizeOnly(text):\n",
    "    return normalizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ref-norm'] = df.apply(lambda x: filterAndNormalize(x['ref']), axis=1)\n",
    "df['hyp-clean-norm'] = df.apply(lambda x: filterAndNormalize(x['hyp-clean']), axis=1)\n",
    "df['hyp-prmpt-norm'] = df.apply(lambda x: filterAndNormalize(x['hyp-prmpt']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcWER(df):\n",
    "    dff = df\n",
    "    wer_cln = jiwer.wer(list(dff['ref']), list(dff['hyp-clean']))\n",
    "    wer_prm = jiwer.wer(list(dff['ref']), list(dff['hyp-prmpt']))\n",
    "    wer_cln_nrm = jiwer.wer(list(dff['ref-norm']), list(dff['hyp-clean-norm']))\n",
    "    wer_prm_nrm = jiwer.wer(list(dff['ref-norm']), list(dff['hyp-prmpt-norm']))\n",
    "\n",
    "    print('clean        : {} %'.format(round(wer_cln*100,4)))\n",
    "    print('prmpt        : {} %'.format(round(wer_prm*100,4)))\n",
    "    print('clean-norm   : {} %'.format(round(wer_cln_nrm*100,4)))\n",
    "    print('prmpt-norm   : {} %'.format(round(wer_prm_nrm*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsp = '-'.join(mdl.split('-')[1:])\n",
    "\n",
    "print('Dataset: ', dts)\n",
    "print('Model  : ', mdl)\n",
    "print('Whisper: ', wsp)\n",
    "\n",
    "calcWER(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
